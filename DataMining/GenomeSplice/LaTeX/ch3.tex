\section{Stabla odlučivanja}
\label{ch:ch3}

Stabla odlučivanja vrlo su popularna metoda dubinske analize podataka zbog svoje jednostavnosti i brzine. Algoritam koristi “podijeli-pa-vladaj” paradigmu i najjednostavnije se može opisati u rekurzivnom obliku. Najprije se odabire atribut koji će biti korijen stabla. Za svaku moguću vrijednost koju taj atribut može imati izvodi se jedna grana. Na ovaj način se skup podataka dijeli u podskupove. Ako jedan od čvorova stabla dobivenih na ovaj način sadrži samo instance jedne klase, zaustavlja se rekurzivni postupak. Inače, nastavljamo dijeliti preostale podskupove prema novim atributima. Odabir atributa nije proizvoljan, želimo odabrati atribute na takav način da stablo bude što manje tako da naš algoritam radi brže. 

Indukcijski zadatak provodi se nad skupom objekata koji su opisani kolekcijom atributa. Svaki atribut mjeri neko važno svojstvo objekta te u pravilu poprima veličinu iz diskretnog skupa vrijednosti. Objekti pripadaju jednoj od dvije ili više međusobno isključivih klasa. Iz skupa podataka za trening u kojem su klase objekata poznate izvode se klasifikacijska pravila. Ukoliko su atributi adekvatni (objekti s istim vrijednostima atributa pripadaju istoj klasi) uvijek je moguće konstruirati stablo odlučivanja koje će ispravno klasificirati sve objekte u trening skupu\cite{Witten01}. 
Međutim, stablo odlučivanja koje ispravno klasificira samo trening skup nije korisno jer zapravo samo izražava podatke koje imamo u tablici (trening skupa) u obliku stabla. Kako bi se  konstruirano stablo moglo primjeniti i na buduće, dotad neviđene podatke (testni skup) stablo odlučivanja mora sadržavati smislene informacije o odnosima atributa objekta i klase tog objekta. 

Računalni znanstvenik John Ross Quinlan dao je značajan doprinos razvoju algoritama za dubinsku analizu temeljenim na stablima odlučivanja\cite{Quinlan01},\cite{Quinlan02},\cite{Wu01}. Istraživanje je potaknuto potrebom da se unaprijede algoritmi sa sposobnošću pronalaska znanja u samim skupovima podataka bez korištenja domenskih eksperata. Prvi iz niza varijanti stabla odlučivanja koje je dizajnirao, ID3, je dizajniran za skupove podataka s velikim brojem objekata i velikim brojem atributa objekta poštujući ograničenje da konstruirano stablo odlučivanja bude jednostavno (kako bi se ubrzao proces generiranja takvog stabla uz minimalne računalne resurse). Posljedica ovakvih ograničenja u dizajnu je da ID3 ne garantira globalno optimalno stablo odlučivanja\cite{Quinlan02}.

\subsection{C4.5 algoritam}

C4.5 je nasljednik algoritma ID3. Kao i prethodni, C4.5 generira klasifikator u obliku stabla odlučivanja, ali može 
kreirati i klasifikator u razumljivijem formatu, u obliku skupa pravila. Pretpostavimo da postoji skup \textit{S} 
slučajeva. C4.5 konstruira stablo na sljedeći način:
\begin{itemize}
   \item Ako svi slučajevi u \textit{S} pripadaju istoj klasi ili je skup mal, stablo je samo čvor s oznakom najčešće klase u skupu \textit{S}
   \item Inače, odabrati test na jednom atributu s dva ili više ishoda i postaviti ovaj test u korijen stabla s granom za svaki ishod. Podijeliti skup \textit{S} na podskupove \textit{S1}, \textit{S2}, ... ovisno o ishodu svakog test te nastaviti s procedurom rekurzivno za svaki podskup.
\end{itemize}

Ova definicija je poprilično općenita jer bi mogli odabrati mnogo različitih testova. C4.5 koristi dvije heuristike za rangiranje testova
\begin{itemize}
   \item \textit{Informacijska vrijednost\footnote{eng. Information Gain} IG}  - minimizira ukupnu entropiju podskupova i
   \item \textit{Omjer dobiti\footnote{eng. Gain Ratio} GR} - dijeli IG sa informacijom iz ishoda testa.
\end{itemize}

Dozvoljeni su i numerički i nominalni atributi. Numberički atributi u pravilu koriste testove s pragovima, te ishodi
ovise o tome jesu li vrijednosti atributa veće ili manje (ili jednake) od praga. Kod atributa s diskretnim vrijednostima
u pravilu testovi imaju jednak broj ishoda kao i broj mogućih vrijednosti tog atributa, ali je moguće i grupiranje 
vrijednosti kako bi se smanjio broj ishoda testa, a samim time i složenost konstruiranog stabla.

Inicijalno konstruirano stablo je podložno pretreniranju (eng. \textit{overfitting}) zbog čega se koristi algoritam podrezivanja stabla. Podrezivanje se odvija od listova prema korijenu. Računa se pesimistična procjena učestalosti pogrešak korištenjem binomne razdiobe za slučaj kada je registrirano E događaja, koji ne pripadaju najčešćoj klasi, u N pokušaja, s korisnički definiranim intervalom pouzdanosti. Za podstablo se sumira procjena pogreške svih grana i uspoređuje sa greškom u slučaju da se cijelo podstablo zamjeni čvorom. Ukoliko potonja greška nije veća stablo se podrezuje. Također, moguće je zamjenjivanje podstabla samo jednom njegovom granom ukoliko to ne povećava pogreške\cite{Wu01}.

\subsubsection{Informacijska vrijednost i omjer dobiti}

\begin{equation}
info(S) = -\mathlarger{\sum}_{j}\frac{|C_{jS}|}{|S|}\cdot\log_{2}\frac{|C_{jS}|}{|S|}
\end{equation}

\begin{equation}
info_{X}(S) = -\mathlarger{\sum}_{i}\frac{|S_{i}|}{|S|}\cdot info(S_{i})
\end{equation}

\begin{equation}
gain(X) = info(S)-info_{X}(S)
\end{equation}

\begin{equation}
split info(X) = -\mathlarger{\sum}_{i}\frac{|S_{i}|}{|S|}\cdot\log_{2}\frac{|S_{i}|}{|S|}
\end{equation}

\begin{equation}
GR(X) = info(S)/split info_(X)
\end{equation}
Two factors are attributed
to this inferior accuracy: the single coverage constraint and the fragmentation problem
[14,7,13].
The single coverage constraint originates from the heuristic nature of the C4.5
method. The construction of C4.5 trees is a recursive process. The process first selects
a feature which is most discriminatory with regard to the entire training data. Then
the process goes to use this feature to split the data into non-overlapping sub-groups
so that each sub-group contains as many samples of the same class as possible. Then
the process is applied to each of these sub-groups of the training data, and iteratively
applied to the resulting sub-groups until all sub-groups contain pure or almost pure class
of samples.
Each sub-group of training data corresponds to a rule with which all samples in this
sub-group satisfy. Because no training data is repeated in these sub-groups, no training
samples can satisfy more than one rule’s conditions. So, every training sample has one
and only one rule in the tree to be satisfied. We call this the single coverage constraint
or mutual exclusivity. Due to this constraint, decision tree methods do not encourage
G. Dong et al. (Eds.): WAIM 2003, LNCS 2762, pp. 254–265, 2003. Springer-Verlag Berlin Heidelberg 2003
Using Rules to Analyse Bio-medical Data: A Comparison between C4.5 and PCL 255
many significant rules to be generated, though there exist lots of significant rules in
bio-medical data particularly for high-dimensional gene expression profiling data [8,18,
17,9]. The small number of significant rules can lead to biased predictions.
The fragmentation problem is that as less and less training data are used to search for
root nodes of sub-trees (sub-groups), a series of many locally important but globally unimportant
rules may be generated. These minor rules can in turn mis-guide the resulting
system, decreasing the accuracy of the trees.
